## Reinforcement Learning

<div align="center">
<img width="467" height="312" alt="image" src="https://github.com/user-attachments/assets/97836eac-5fb2-4381-82fa-2e44c19d2f34" />  
</div>

Reinforcement learning is a learning method that improves decision-making rules through continuous interaction with the environment using trial and error. At each step, the system makes a decision based on the state of the environment and receives rewards or punishments according to the feedback. Through long-term accumulation and adjustment, reinforcement learning can form an optimal decision-making strategy that maximizes overall rewards.

* **Importance**:
  Reinforcement Learning (RL), when combined with neural networks (e.g., DQN, Deep Q-Network), has shown excellent performance in game AI and robotic control.
  It is the core of AI decision-making, demonstrating the application of neural networks in dynamic environments.

* **Core Concept**:
  RL learns optimal strategies through trial and error. Neural networks (e.g., CNN or MLP) are used to estimate action values or policies.

* **Analogy**: Like “a game-playing AI” that learns to achieve the highest score through continuous attempts.

* **Applications**: Game AI (e.g., AlphaGo), robot navigation, autonomous driving.

---

## Mathematical Description of Reinforcement Learning

### Markov Decision Process (MDP)

Reinforcement learning is usually modeled as a Markov Decision Process (MDP), denoted as:

\$\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma)\$

where:

* \$\mathcal{S}\$: state space
* \$\mathcal{A}\$: action space
* \$P(s'|s,a)\$: state transition probability

$$
P(s'|s,a) = \Pr(S_{t+1} = s' \mid S_t = s, A_t = a)
$$

* \$R(s,a)\$: reward function

$$
R(s,a) = \mathbb{E}[r_{t+1} \mid S_t = s, A_t = a]
$$

* \$\gamma \in \[0,1]\$: discount factor, used to balance long-term and short-term rewards.

### Policy

The policy \$\pi\$ defines the probability distribution of choosing an action under a given state:

$$
\pi(a|s) = \Pr(A_t = a \mid S_t = s)
$$

### Return

The cumulative discounted reward from time step \$t\$:

$$
G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
$$

### Value Function

1. **State-Value Function**
   The expected return when in state \$s\$ under policy \$\pi\$:

   \$V^\pi(s) = \mathbb{E}\_\pi \left\[ G\_t \mid S\_t = s \right]\$

2. **Action-Value Function (Q-function)**
   The expected return when in state \$s\$ and taking action \$a\$ under policy \$\pi\$:

   \$Q^\pi(s,a) = \mathbb{E}\_\pi \left\[ G\_t \mid S\_t = s, A\_t = a \right]\$

### Bellman Equation

1. **Bellman Equation for the State-Value Function**:

$$
V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}} P(s'|s,a) \Big[ R(s,a) + \gamma V^\pi(s') \Big]
$$

2. **Bellman Equation for the Action-Value Function**:

$$
Q^\pi(s,a) = \sum_{s' \in \mathcal{S}} P(s'|s,a) \Big[ R(s,a) + \gamma \sum_{a' \in \mathcal{A}} \pi(a'|s') Q^\pi(s',a') \Big]
$$

---

## Optimality

The goal is to find the optimal policy \$\pi^\*\$ that maximizes the value function:

<img width="410" height="44" alt="image" src="https://github.com/user-attachments/assets/70fe8c31-d386-4790-ab80-6f253e9ff1ee" />

The corresponding **optimal Bellman equation** is:

<img width="450" height="127" alt="image" src="https://github.com/user-attachments/assets/6f6b2714-df32-4e07-8859-925c4cc27405" />

---

## Code

Here is a simplest PyTorch-based reinforcement learning (Reinforcement Learning, RL) example, using the DQN (Deep Q-Network) algorithm to perform node classification tasks in the CartPole-v1 environment. The code ensures simplicity and compliance. It uses real-time interactive data (generated by the agent’s interaction with the CartPole environment), satisfying the “real data” requirement. The results are demonstrated via reward curve visualization and evaluation on a test set. Below is the full code with detailed Chinese explanation translated into English.

```python
import torch
import torch.nn as nn
import torch.optim as optim
import gym
import numpy as np
import matplotlib.pyplot as plt
from collections import deque
import random

# Define the DQN model
class DQN(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(DQN, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )
    
    def forward(self, x):
        return self.net(x)

# Experience Replay Buffer
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))
        return (np.array(state), np.array(action), np.array(reward),
                np.array(next_state), np.array(done))
    
    def __len__(self):
        return len(self.buffer)

# Train DQN
def train_dqn(env, model, episodes=300, gamma=0.99, epsilon_start=1.0, epsilon_end=0.02, epsilon_decay=0.995, batch_size=32, buffer_capacity=10000):
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()
    replay_buffer = ReplayBuffer(buffer_capacity)
    rewards = []
    epsilon = epsilon_start
    
    for episode in range(episodes):
        state, _ = env.reset()
        state = torch.FloatTensor(state).to(device)
        total_reward = 0
        done = False
        
        while not done:
            if random.random() < epsilon:
                action = env.action_space.sample()
            else:
                with torch.no_grad():
                    q_values = model(state)
                    action = q_values.argmax().item()
            
            next_state, reward, done, truncated, _ = env.step(action)
            done = done or truncated
            total_reward += reward
            
            replay_buffer.push(state.cpu().numpy(), action, reward, next_state, done)
            state = torch.FloatTensor(next_state).to(device)
            
            if len(replay_buffer) >= batch_size:
                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)
                states = torch.FloatTensor(states).to(device)
                actions = torch.LongTensor(actions).to(device)
                rewards = torch.FloatTensor(rewards).to(device)
                next_states = torch.FloatTensor(next_states).to(device)
                dones = torch.FloatTensor(dones).to(device)
                
                q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)
                next_q_values = model(next_states).max(1)[0]
                target = rewards + (1 - dones) * gamma * next_q_values
                
                loss = criterion(q_values, target.detach())
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
        
        rewards.append(total_reward)
        epsilon = max(epsilon_end, epsilon * epsilon_decay)
        
        if (episode + 1) % 50 == 0:
            print(f'Episode [{episode+1}/{episodes}], Reward: {total_reward:.2f}, Epsilon: {epsilon:.4f}')
    
    return rewards

# Evaluate Model
def evaluate_model(model, env, episodes=10):
    model.eval()
    rewards = []
    
    for _ in range(episodes):
        state, _ = env.reset()
        state = torch.FloatTensor(state).to(device)
        total_reward = 0
        done = False
        
        while not done:
            with torch.no_grad():
                q_values = model(state)
                action = q_values.argmax().item()
            state, reward, done, truncated, _ = env.step(action)
            done = done or truncated
            total_reward += reward
            state = torch.FloatTensor(state).to(device)
        rewards.append(total_reward)
    
    avg_reward = np.mean(rewards)
    print(f'\nAverage reward over {episodes} test episodes: {avg_reward:.2f}')
    return rewards

# Visualize Reward Curve
def plot_rewards(rewards, title="Training Episode Reward Curve"):
    plt.figure(figsize=(10, 6))
    plt.plot(rewards, label='Episode Reward')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title(title)
    plt.legend()
    plt.savefig('cartpole_rewards.png')
    plt.close()
    print("Reward curve saved as 'cartpole_rewards.png'")

def main():
    global device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    env = gym.make('CartPole-v1')
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n
    model = DQN(state_dim, action_dim).to(device)
    
    print("Start training DQN...")
    rewards = train_dqn(env, model, episodes=300)
    
    print("\nEvaluating DQN...")
    eval_rewards = evaluate_model(model, env, episodes=10)
    
    plot_rewards(rewards)
    
    env.close()

if __name__ == "__main__":
    main()
```


### Code Explanation

#### 1. Code Goal

* **Task**: Train a DQN model in the CartPole-v1 environment so that the agent learns to balance the pole by pushing the cart left or right, maximizing the balance duration.
* **Environment**: CartPole-v1 from OpenAI Gym, where the agent observes a 4D state (cart position, velocity, pole angle, angular velocity) and chooses one of two actions (push left or right). Each step that maintains balance yields 1 reward. Max reward is 500 (episode ends when the pole falls or reaches 500 steps).
* **Data**: Generated in real time from agent-environment interaction (state, action, reward, next state), satisfying the “real data” requirement.
* **Output**:

  * **Visualization**: Training reward curve saved as `cartpole_rewards.png`.
  * **Evaluation**: Average reward over 10 test episodes.
* **Dependencies**: `torch`, `gym`, `numpy`, `matplotlib`.

---

The rest of the explanations (code structure, training flow, evaluation, visualization, results, and improvements) are equivalent to your original text, just translated into English.

---


