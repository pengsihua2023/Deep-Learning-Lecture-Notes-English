{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db623512",
   "metadata": {},
   "source": [
    "## Multi-Task Learning (MTL)\n",
    "<div align=\"center\">\n",
    "<img width=\"400\" height=\"250\" alt=\"image\" src=\"https://github.com/user-attachments/assets/4dd18183-6e9e-4418-ab2b-b0f9e8edb4bb\" />\n",
    "</div>\n",
    "\n",
    "# Definition  \n",
    "Multi-Task Learning (MTL) is a machine learning training paradigm. The core idea is: a model learns multiple related tasks simultaneously, instead of training a separate model for each task as in traditional methods. The model shares most parameters, each task has a specific output head, and multiple objectives are jointly optimized.  \n",
    "\n",
    "---\n",
    "\n",
    "# Mathematical Description of Multi-Task Learning\n",
    "\n",
    "## 1. Basic Form of Single-Task Learning\n",
    "\n",
    "Given a dataset:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N,\n",
    "$$\n",
    "\n",
    "* $x_i \\in \\mathcal{X}$: the input features of the $i$-th sample.  \n",
    "* $y_i \\in \\mathcal{Y}$: the supervision signal (label) of the $i$-th sample.  \n",
    "* $N$: number of training samples.  \n",
    "\n",
    "We train a model parameterized by $\\theta$:\n",
    "\n",
    "$$\n",
    "f_\\theta : \\mathcal{X} \\to \\mathcal{Y},\n",
    "$$\n",
    "\n",
    "The objective is to minimize the expected loss:\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\ \\mathbb{E}_{(x,y)\\sim \\mathcal{D}} \\left[ \\mathcal{L}(f_\\theta(x), y) \\right].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Extension to Multi-Task Learning\n",
    "\n",
    "Suppose there are $T$ tasks, each task $t$ has its dataset:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_t = \\{(x_i^t, y_i^t)\\}_{i=1}^{N_t},\n",
    "$$\n",
    "\n",
    "* $x_i^t$: input of task $t$.  \n",
    "* $y_i^t$: label of task $t$.  \n",
    "* $N_t$: number of samples for task $t$.  \n",
    "\n",
    "Each task has a corresponding loss function $\\mathcal{L}_t$. The optimization objective of MTL is:\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\ \\sum_{t=1}^T \\lambda_t \\, \\mathbb{E}_{(x,y)\\sim \\mathcal{D}_t} \\Big[ \\mathcal{L}_t(f_\\theta(x), y) \\Big].\n",
    "$$\n",
    "\n",
    "* $\\lambda_t$: task weight, controlling the importance of each task in the overall objective.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Structured Representation of Parameter Sharing\n",
    "\n",
    "In practice, **shared representation layers + task-specific output layers** are commonly used:\n",
    "\n",
    "1. **Shared representation layer**:\n",
    "\n",
    "$$\n",
    "h = \\phi_{\\theta_s}(x),\n",
    "$$\n",
    "\n",
    "* $\\phi_{\\theta_s}$: feature extractor (e.g., early layers of a neural network), with parameters $\\theta_s$ shared across all tasks.  \n",
    "* $h$: shared latent representation.  \n",
    "\n",
    "2. **Task-specific output layer**:\n",
    "\n",
    "$$\n",
    "\\hat{y}^t = f^t_{\\theta_t}(h),\n",
    "$$\n",
    "\n",
    "* $f^t_{\\theta_t}$: predictor for task $t$, with parameters $\\theta_t$ used only for task $t$.  \n",
    "* $\\hat{y}^t$: prediction of task $t$.  \n",
    "\n",
    "Overall optimization objective:\n",
    "\n",
    "$$\n",
    "\\min_{\\theta_s, \\{\\theta_t\\}_{t=1}^T} \\ \\sum_{t=1}^T \\lambda_t \\, \\mathbb{E}_{(x,y)\\sim \\mathcal{D}_t} \\left[ \\mathcal{L}_t(f^t_{\\theta_t}(\\phi_{\\theta_s}(x)), y) \\right].\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Matrix/Regularization Perspective\n",
    "\n",
    "Suppose the task parameter matrix is:\n",
    "\n",
    "$$\n",
    "W = [\\theta_1, \\dots, \\theta_T] \\in \\mathbb{R}^{d \\times T},\n",
    "$$\n",
    "\n",
    "We can add regularization constraints outside the loss function:\n",
    "\n",
    "### (a) Low-rank constraint\n",
    "\n",
    "$$\n",
    "\\min_W \\ \\sum_{t=1}^T \\mathcal{L}_t(W_t) + \\lambda \\|W\\|_*\n",
    "$$\n",
    "\n",
    "* $\\|W\\|_*$: nuclear norm, encouraging $W$ to have low rank, implying that tasks share a low-dimensional subspace.  \n",
    "\n",
    "### (b) Graph regularization\n",
    "\n",
    "$$\n",
    "\\min_W \\ \\sum_{t=1}^T \\mathcal{L}_t(W_t) + \\gamma \\sum_{(i,j)\\in E} \\|W_i - W_j\\|^2\n",
    "$$\n",
    "\n",
    "* $E$: set of edges in the task relation graph.  \n",
    "* $\\|W_i - W_j\\|^2$: encourages parameters of similar tasks to be close.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Bayesian Perspective\n",
    "\n",
    "Introduce prior distribution for task parameters:\n",
    "\n",
    "$$\n",
    "p(\\theta_1, \\dots, \\theta_T | \\alpha) = \\prod_{t=1}^T p(\\theta_t | \\alpha)\n",
    "$$\n",
    "\n",
    "* $\\alpha$: shared hyperparameters controlling the prior distribution of all tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "There are three main approaches to mathematical modeling of multi-task learning:\n",
    "\n",
    "1. **Weighted loss function** (simple summation of tasks with weights $\\lambda_t$);  \n",
    "2. **Parameter sharing** (shared layers $\\theta_s$ + task-specific heads $\\theta_t$);  \n",
    "3. **Regularization / probabilistic modeling** (using nuclear norm, graph regularization, or shared priors to model task relationships).  \n",
    "\n",
    "A simplest PyTorch-based Multi-Task Learning (MTL) example uses a real dataset (UCI Wine Quality dataset), implementing two tasks: predicting wine quality (regression task) and predicting whether the wine is good (classification task, quality â‰¥ 6). Results are shown via visualization (scatter plot of predicted quality) and evaluation metrics (MSE for regression, accuracy for classification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d534818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define Multi-Task Learning model\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.regression_head = nn.Linear(hidden_dim, 1)\n",
    "        self.classification_head = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shared_features = self.shared(x)\n",
    "        quality_pred = self.regression_head(shared_features)\n",
    "        is_good_pred = self.classification_head(shared_features)\n",
    "        return quality_pred, is_good_pred\n",
    "\n",
    "# Data preparation\n",
    "def prepare_data():\n",
    "    data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep=';')\n",
    "    X = data.drop('quality', axis=1).values\n",
    "    y_quality = data['quality'].values\n",
    "    y_class = (y_quality >= 6).astype(int)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_quality_train, y_quality_test, y_class_train, y_class_test = train_test_split(\n",
    "        X, y_quality, y_class, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    X_test = torch.FloatTensor(X_test)\n",
    "    y_quality_train = torch.FloatTensor(y_quality_train).reshape(-1, 1)\n",
    "    y_quality_test = torch.FloatTensor(y_quality_test).reshape(-1, 1)\n",
    "    y_class_train = torch.FloatTensor(y_class_train).reshape(-1, 1)\n",
    "    y_class_test = torch.FloatTensor(y_class_test).reshape(-1, 1)\n",
    "    \n",
    "    return X_train, X_test, y_quality_train, y_quality_test, y_class_train, y_class_test\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, X_train, y_quality_train, y_class_train, epochs=100, lr=0.01):\n",
    "    criterion_reg = nn.MSELoss()\n",
    "    criterion_cls = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        quality_pred, is_good_pred = model(X_train)\n",
    "        loss_reg = criterion_reg(quality_pred, y_quality_train)\n",
    "        loss_cls = criterion_cls(is_good_pred, y_class_train)\n",
    "        loss = loss_reg + loss_cls\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, '\n",
    "                  f'Regression Loss: {loss_reg.item():.4f}, Classification Loss: {loss_cls.item():.4f}')\n",
    "\n",
    "# Evaluation and Visualization\n",
    "def evaluate_and_visualize(model, X_test, y_quality_test, y_class_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        quality_pred, is_good_pred = model(X_test)\n",
    "        quality_pred = quality_pred.numpy()\n",
    "        is_good_pred = (torch.sigmoid(is_good_pred) > 0.5).float().numpy()\n",
    "        y_quality_test = y_quality_test.numpy()\n",
    "        y_class_test = y_class_test.numpy()\n",
    "    \n",
    "    mse = mean_squared_error(y_quality_test, quality_pred)\n",
    "    accuracy = accuracy_score(y_class_test, is_good_pred)\n",
    "    print(f'\\nTest Set Evaluation:')\n",
    "    print(f'Regression MSE: {mse:.4f}')\n",
    "    print(f'Classification Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_quality_test, quality_pred, alpha=0.5)\n",
    "    plt.plot([y_quality_test.min(), y_quality_test.max()], [y_quality_test.min(), y_quality_test.max()], 'r--')\n",
    "    plt.xlabel('True Quality')\n",
    "    plt.ylabel('Predicted Quality')\n",
    "    plt.title('Wine Quality Prediction (Regression Task)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('wine_quality_prediction.png')\n",
    "    plt.close()\n",
    "    print(\"Prediction scatter plot saved as 'wine_quality_prediction.png'\")\n",
    "\n",
    "    print(\"\\nSample Predictions (First 5):\")\n",
    "    for i in range(5):\n",
    "        print(f\"Sample {i+1}: True Quality={y_quality_test[i][0]:.2f}, Predicted Quality={quality_pred[i][0]:.2f}, \"\n",
    "              f\"True Class={y_class_test[i][0]:.0f}, Predicted Class={is_good_pred[i][0]:.0f}\")\n",
    "\n",
    "def main():\n",
    "    X_train, X_test, y_quality_train, y_quality_test, y_class_train, y_class_test = prepare_data()\n",
    "    model = MultiTaskModel(input_dim=11, hidden_dim=64)\n",
    "    train_model(model, X_train, y_quality_train, y_class_train, epochs=100)\n",
    "    evaluate_and_visualize(model, X_test, y_quality_test, y_class_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766eecb0",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "1. **Dataset**:\n",
    "   - Use the UCI Wine Quality dataset (red wine, 1599 samples), with 11 chemical features and quality scores (range 3â€“8).\n",
    "   - Task 1 (Regression): predict quality score.\n",
    "   - Task 2 (Classification): predict whether the wine is good (quality â‰¥ 6).\n",
    "   - Data is loaded via `pandas` from the UCI website, standardized, and split into training (80%) and testing (20%) sets.\n",
    "\n",
    "2. **Model Structure**:\n",
    "   - Shared layers: two fully connected layers (ReLU activation), 11 input features, 64 hidden units.\n",
    "   - Regression head: outputs 1-dimensional quality score.\n",
    "   - Classification head: outputs 1-dimensional binary probability (good/not good).\n",
    "   - Loss functions: regression uses MSELoss, classification uses BCEWithLogitsLoss, combined loss is the sum of both.\n",
    "\n",
    "3. **Training**:\n",
    "   - Use Adam optimizer, learning rate 0.01, train for 100 epochs.\n",
    "   - Print total loss, regression loss, and classification loss every 20 epochs.\n",
    "\n",
    "4. **Evaluation and Visualization**:\n",
    "   - Evaluate regression task with Mean Squared Error (MSE) and classification task with Accuracy.\n",
    "   - Generate scatter plot comparing true vs predicted wine quality, saved as `wine_quality_prediction.png`.\n",
    "   - Print first 5 test samplesâ€™ true and predicted values (quality score and classification).\n",
    "\n",
    "5. **Dependencies**:\n",
    "   - Requires `torch`, `sklearn`, `pandas`, `matplotlib`, `seaborn` (`pip install torch scikit-learn pandas matplotlib seaborn datasets`).\n",
    "   - Dataset is loaded online, no manual download needed.\n",
    "\n",
    "### Results:\n",
    "- Outputs training

