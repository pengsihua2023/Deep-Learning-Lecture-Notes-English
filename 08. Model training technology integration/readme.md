
## Integration of Deep Learning Model Training Techniques

* Key Technologies and Techniques

  * [Automatic Mixed Precision (AMP)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Automatic%20Mixed%20Precision%20(AMP).md)
  * [Curriculum Learning](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Curriculum%20Learning.md)
  * [Optuna Hyperparameter Optimization Method](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Optuna%20Hyperparameter%20Optimization%20Method.md)
  * [Ray Tune Hyperparameter Optimization Method](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Ray%20Tune%20Hyperparameter%20Optimization%20Method.md)
  * [Two methods for dealing with class imbalance: weighted loss function and oversampling](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Two%20methods%20for%20dealing%20with%20class%20imbalance%3A%20weighted%20loss%20function%20and%20oversampling.md)
  * [Min-Max Normalization](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Min-Max%20Normalization.md)
  * [Z-score Standardization](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Z-score%20Standardization.md)
  * [Mixed Precision Training](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Mixed%20Precision%20Training.md)
  * [Multi-GPU Parallel Training (Distributed Data Parallel, DDP)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Multi-GPU%20Parallel%20Training%20(Distributed%20Data%20Parallel%2C%20DDP).md)
  * [Gradient Clipping](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Gradient%20Clipping.md)
  * [Gradient Accumulation](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Gradient%20Accumulation.md)
  * [Efficient Attention Computation (Flash Attention)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Efficient%20Attention%20Computation%20(Flash%20Attention).md)
  * [Hyperparameter Search (Bayesian Optimization)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Hyperparameter%20Search%20(Bayesian%20Optimization).md)
  * [Multi-Model Ensemble (Ensemble Learning)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Multi-Model%20Ensemble%20(Ensemble%20Learning).md)

* Overview of Regularization Techniques

  * [L1 Norm Regularization (L1 Regularization)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/L1%20Norm%20Regularization%20(L1%20Regularization).md)
  * [L2 Norm Regularization (L2 Regularization, Weight Decay)](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/L2%20Norm%20Regularization.md)
  * Batch Normalization
  * Layer Normalization
  * [Early Stopping](https://github.com/pengsihua2023/Deep-Learning-Lecture-Notes-English/blob/main/08.%20Model%20training%20technology%20integration/Early%20Stopping.md)
  * Noise Injection to Inputs/Weights
  * Dropout (Randomly Dropping Neurons)

* Overview of Learning Rate Adjustment Methods

  * Dynamic Learning Rate Adjustment (Learning Rate Scheduling)
  * Automatic Learning Rate Reduction Based on Loss Monitoring (ReduceLROnPlateau)
  * Adaptive Learning Rate (Adam Optimizer)
  * Adaptive Learning Rate (RMSprop)

* Overview of Optimizers

  * Adam Optimizer
  * Adam Variant (AdamW)
  * SGD Optimizer (Stochastic Gradient Descent)
  * RMSProp Optimizer (Root Mean Square Propagation)
  * Adagrad Optimizer (Adaptive Gradient Algorithm)
  * Adadelta Optimizer
  * AMSGrad Optimizer
  * Nadam Optimizer
  * L-BFGS Optimizer (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)
  * Rprop Optimizer (Resilient Backpropagation)
  * SparseAdam Optimizer
  * ASGD Optimizer (Averaged Stochastic Gradient Descent)

* Overview of Initialization Methods

  * Uniform/Normal Distribution Initialization (Xavier/Glorot Initialization)
  * Initialization Considering ReLU Variance (He Initialization)
  * Uniform Distribution Initialization
  * Normal Distribution Initialization
  * Uniform Variant of He Initialization (Kaiming Uniform)
  * Layer-Sequential Unit-Variance Initialization (LSUV)
  * Zero Initialization

* Overview of Model Fine-Tuning Techniques

  * Low-Rank Adaptation (LoRA)
  * Prompt Tuning
  * Restarted LoRA (ReLoRA)
  * Quantization + LoRA (QLoRA)
  * Differential Pruning (Diff Pruning)
  * Adding Small Adapter Layers (Adapter Modules)
  * Model Pruning (Removing Unimportant Weights)

* Summary of Model Evaluation Methods

* Overview of Model Interpretation Methods

  * SHAP
  * LIME
  * Saliency Maps
  * Integrated Gradients
  * Grad-CAM
  * DeepLIFT
  * t-SNE
  * Attention Visualization
