# Prefix Tuning Fine-Tuning

## üìñ 1. Definition

**Prefix Tuning** is a Parameter-Efficient Fine-Tuning (PEFT) method.
Core idea:

* Freeze all parameters of the pre-trained language model;
* Insert a small set of **trainable ‚Äúprefix tokens‚Äù** before the **attention mechanism input** of each Transformer layer;
* During training, only update these prefix parameters, without modifying the original model parameters.

In this way, Prefix Tuning can greatly reduce the number of trainable parameters, and for different tasks, only a small prefix needs to be stored to achieve transfer.



## üìñ 2. Mathematical Formulation

Let the input of a Transformer attention layer be **query**, **key**, and **value**:

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V
$$

In Prefix Tuning:

* For each layer \$l\$, introduce prefix key and prefix value:

$$
K' = [P_k^l; K], \quad V' = [P_v^l; V]
$$

Where:

* \$P\_k^l, P\_v^l\$ are trainable prefix parameters, usually generated by a small MLP from prefix embeddings;
* ‚Äú;‚Äù denotes concatenation.

Thus, the attention becomes:

$$
\text{Attention}(Q, K', V') = \text{softmax}\left(\frac{Q {K'}^T}{\sqrt{d_k}}\right) V'
$$

During training, only \${P\_k^l, P\_v^l}\$ are updated, while the original \$W\_Q, W\_K, W\_V\$ remain frozen.

---

## üìñ 3. Minimal Code Example

Here is a minimal **Prefix Tuning** example in PyTorch (adding prefix in a Transformer layer):

```python
import torch
import torch.nn as nn

class PrefixTuningAttention(nn.Module):
    def __init__(self, d_model=128, n_heads=4, prefix_len=5):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, n_heads)
        self.prefix_len = prefix_len

        # Trainable prefix parameters (prefix_len prefix tokens)
        self.prefix_key = nn.Parameter(torch.randn(prefix_len, 1, d_model))
        self.prefix_value = nn.Parameter(torch.randn(prefix_len, 1, d_model))

    def forward(self, x):
        # x: [seq_len, batch, d_model]
        seq_len, batch, d_model = x.shape

        # Expand prefix to batch dimension
        pk = self.prefix_key.expand(-1, batch, -1)  # [prefix_len, batch, d_model]
        pv = self.prefix_value.expand(-1, batch, -1)

        # Original Q, K, V
        q = k = v = x

        # Concatenate prefix to K, V
        k = torch.cat([pk, k], dim=0)
        v = torch.cat([pv, v], dim=0)

        # Attention computation
        out, _ = self.attn(q, k, v)
        return out

# ===== Test =====
x = torch.randn(10, 2, 128)  # [seq_len=10, batch=2, hidden=128]
layer = PrefixTuningAttention()
out = layer(x)
print("Output shape:", out.shape)
```

Output:

```
Output shape: torch.Size([10, 2, 128])
```

This shows that the Prefix Tuning layer runs correctly.


## üìñ Summary

* **Prefix Tuning**: Introduces prefix key/value before the attention layer, without modifying original weights.
* **Advantages**: Greatly reduces trainable parameters, allows easy multi-task sharing of pre-trained models.
* **Core Formula**:

$$
K' = [P_k; K], \quad V' = [P_v; V]
$$

---

Now, here is a full example of **Prefix Tuning fine-tuning with Hugging Face PEFT** on BERT, demonstrated with a small text classification task (SST-2 sentiment classification).

## üìñ Prefix Tuning with Hugging Face PEFT

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
from peft import get_peft_model, PrefixTuningConfig, TaskType

# 1. Load pre-trained BERT model and tokenizer
model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# 2. Define Prefix Tuning configuration
peft_config = PrefixTuningConfig(
    task_type=TaskType.SEQ_CLS,   # Task type: sequence classification
    num_virtual_tokens=20,        # Length of prefix (tunable)
    encoder_hidden_size=768       # Hidden dim of BERT
)

# 3. Wrap model into Prefix Tuning model
model = get_peft_model(model, peft_config)

# 4. Load dataset (SST-2 sentiment classification)
dataset = load_dataset("glue", "sst2")

def preprocess(example):
    return tokenizer(example["sentence"], truncation=True, padding="max_length", max_length=128)

encoded_dataset = dataset.map(preprocess, batched=True)
encoded_dataset = encoded_dataset.rename_column("label", "labels")
encoded_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

train_dataset = encoded_dataset["train"].shuffle(seed=42).select(range(2000))  # small sample for demo
eval_dataset = encoded_dataset["validation"].shuffle(seed=42).select(range(500))

# 5. Training arguments
training_args = TrainingArguments(
    output_dir="./prefix_tuning_out",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    learning_rate=5e-4,
    evaluation_strategy="steps",
    eval_steps=50,
    logging_steps=50,
    save_strategy="no",
    fp16=True
)

# 6. Trainer API training
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

trainer.train()

# 7. Test inference
text = "The movie was fantastic!"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
pred = outputs.logits.argmax(dim=-1).item()
print(f"Input: {text} ‚Üí Predicted class: {pred}")
```



## üìñ Explanation

1. **Prefix Tuning configuration**

   * `num_virtual_tokens=20` means injecting 20 prefix tokens before each attention layer.
   * During training, only prefix parameters are updated; all other BERT weights remain frozen.

2. **Dataset**

   * Uses the `GLUE/SST-2` dataset (binary classification: positive / negative).
   * Only a small subset of data is used here for quick demonstration.

3. **Training**

   * Only prefix parameters are updated, greatly reducing memory and parameter requirements.
   * After a few epochs, validation accuracy starts to improve.

