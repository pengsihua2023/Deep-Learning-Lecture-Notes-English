
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)\n",
    "\n",
    "The Variational Autoencoder (VAE) is a generative deep learning model proposed by Kingma and Welling in 2013. It is a variant of the Autoencoder but incorporates the concept of Variational Inference, enabling it to generate new data rather than merely compressing and reconstructing inputs. The primary goal of VAE is to learn a latent representation of data and generate samples similar to the training data by sampling from the latent space.\n",
    "\n",
    "### Core Components of VAE Include:\n",
    "- **Encoder**: Maps input data x to the distribution parameters of the latent space (typically the mean μ and variance σ² of a Gaussian distribution).\n",
    "- **Sampling**: Samples latent variables z from the latent distribution using the reparameterization trick to make the sampling process differentiable.\n",
    "- **Decoder**: Reconstructs output data x' from the latent variable z, aiming to make x' as close as possible to x.\n",
    "- **Loss Function**: Combines reconstruction loss (e.g., MSE) and KL divergence (Kullback-Leibler divergence) to regularize the latent distribution, making it close to a prior distribution (typically a standard normal distribution).\n",
    "\n",
    "The advantage of VAE lies in its ability to create a continuous latent space, supporting interpolation and generating new samples. It is commonly used in image generation, data augmentation, and other fields. Compared to GANs (Generative Adversarial Networks), VAE training is more stable, but the generated samples may be blurrier.\n",
    "\n",
    "![Figure](https://github.com/user-attachments/assets/d8b5e82e-5b83-41d9-8b3c-521a3aeeb38e)\n",
    "### Mathematical Description\n",
    "The goal of VAE is to maximize the marginal likelihood $p(x)$, which is typically intractable to compute directly. Therefore, the Evidence Lower Bound (ELBO) is used as a proxy optimization objective. Assumptions:\n",
    "- Prior distribution: $p(z) = N(0, I)$ (standard normal distribution).\n",
    "- Approximate posterior: $q(z|x) = N(\\mu, \\sigma^2I)$, parameterized by the encoder, where $\\mu$ and $\\sigma$ are computed from x by a neural network.\n",
    "- Generative model: $p(x|z)$, parameterized by the decoder, typically assumed as $p(x|z) = N(\\text{decoder output}, I)$ or a Bernoulli distribution (for binary data).\n",
    "\n",
    "The ELBO is mathematically expressed as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{q_\\phi(z|x)} [\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) \\| p(z))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ represents the decoder parameters, and $\\phi$ represents the encoder parameters.\n",
    "- The first term is the reconstruction loss: measures the accuracy of reconstructing x from z, typically implemented as negative log-likelihood (e.g., MSE for continuous data: $\\|x - \\hat{x}\\|^2 / 2$).\n",
    "- The second term is the KL divergence: regularizes $q(z|x)$ to be close to $p(z)$, with the formula (assuming Gaussian distribution):\n",
    "\n",
    "$$\n",
    "D_{KL}(q(z|x) \\| p(z)) = -\\frac{1}{2} \\sum_{j=1}^J (1 + \\log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2)\n",
    "$$\n",
    "\n",
    "Where $J$ is the dimension of the latent space.\n",
    "\n",
    "To enable gradient propagation, the reparameterization trick is used: $z = \\mu + \\sigma \\odot \\epsilon$, where $\\epsilon \\sim N(0, I)$.\n",
    "\n",
    "Optimization process: Maximize the ELBO (equivalent to minimizing the negative ELBO) using stochastic gradient descent.\n",
    "\n",
    "## Implementation Notes\n",
    "The following is a minimal VAE implementation using PyTorch for the MNIST dataset (28x28 grayscale images). It uses a simple multilayer perceptron (MLP) as the encoder and decoder, with a latent dimension of 2 (for visualization purposes). The code is consolidated into a single module, including model definition, loss function, training loop, and sample generation. Running it requires PyTorch and torchvision (`pip install torch torchvision`).\n",
    "\n",
    "- **Runtime Environment**: Ensure GPU support for faster training (the code automatically detects the device).\n",
    "- **Extensions**: This is a simplified version for understanding VAE principles. In practice, convolutional neural networks (CNNs) can replace MLPs, the latent dimension can be increased, or hyperparameters can be tuned for better performance.\n",
    "- **Sample Generation**: After training, uncomment the `save_image` section to save generated MNIST image samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Set hyperparameters\n",
    "input_dim = 28 * 28  # MNIST image size\n",
    "hidden_dim = 400\n",
    "latent_dim = 2  # Latent space dimension\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h))  # Output in [0,1], suitable for MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')  # Reconstruction loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())  # KL divergence\n",
    "    return BCE + KLD\n",
    "\n",
    "# Main function: Training and generation\n",
    "def main():\n",
    "    # Initialize model and optimizer\n",
    "    model = VAE()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "    # Generate samples\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(64, latent_dim)  # Random sampling\n",
    "        samples = model.decode(z).view(64, 1, 28, 28)\n",
    "        # Uncomment the following to save generated images\n",
    "        # from torchvision.utils import save_image\n",
    "        # save_image(samples, 'samples.png')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

