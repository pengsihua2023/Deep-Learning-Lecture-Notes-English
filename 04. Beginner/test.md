æ˜ç™½äº† ğŸ‘ã€‚åœ¨ GitHub ä¸Šçš„ README æˆ–æ–‡æ¡£é‡Œï¼Œé€šå¸¸éœ€è¦ä½¿ç”¨ **è¡Œå†… `$...$`** æˆ– **å—çº§å…¬å¼ `$$...$$`** çš„ LaTeX è¯­æ³•æ‰èƒ½æ­£ç¡®æ˜¾ç¤ºã€‚ä¸‹é¢æˆ‘ç»™ä½ ä¸€ä¸ª **é€‚åˆ GitHub æ˜¾ç¤ºçš„è‹±æ–‡ç‰ˆå…¨è¿æ¥ç½‘ç»œæ•°å­¦æè¿°**ï¼š

---

## Fully Connected Neural Network (Mathematical Description)

A fully connected network (also called a dense network) consists of multiple layers of neurons. Each neuron in one layer is connected to all neurons in the next layer.

### Notation

* Input vector:

  $$
  \mathbf{x} \in \mathbb{R}^{d}
  $$

  where \$d\$ is the input dimension.

* Layers:
  Suppose the network has \$L\$ layers (excluding the input).
  The \$l\$-th layer (\$l = 1,2,\dots,L\$) has \$n\_l\$ neurons.

* Parameters:

  * Weight matrix for layer \$l\$:

    $$
    W^{(l)} \in \mathbb{R}^{n_l \times n_{l-1}}
    $$
  * Bias vector for layer \$l\$:

    $$
    \mathbf{b}^{(l)} \in \mathbb{R}^{n_l}
    $$

* Activation function (elementwise):

  $$
  \sigma(\cdot)
  $$

---

### Forward Propagation

1. **Pre-activation (linear transformation):**

   $$
   \mathbf{z}^{(l)} = W^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}, \quad \mathbf{a}^{(0)} = \mathbf{x}
   $$

2. **Activation:**

   $$
   \mathbf{a}^{(l)} = \sigma(\mathbf{z}^{(l)})
   $$

---

### Output

For the last layer \$L\$, the network output is:

$$
\mathbf{y} = \mathbf{a}^{(L)}
$$

* For **regression**: the last activation may be the identity function.
* For **binary classification**: a sigmoid activation is often used.
* For **multi-class classification**: a softmax activation is typically applied.

---

è¿™æ ·å†™çš„è¯ï¼Œåœ¨ GitHub çš„ Markdown æ¸²æŸ“ä¸­ä¼šæ­£å¸¸æ˜¾ç¤ºå…¬å¼ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠå®ƒæ•´ç†æˆä¸€ä¸ª **æœ€å°çš„ GitHub README ç¤ºä¾‹**ï¼ˆå¸¦æ ‡é¢˜å’Œå…¬å¼ï¼‰ï¼Œä½ ç›´æ¥æ‹·è´è¿‡å»å°±èƒ½ç”¨ï¼Ÿ
